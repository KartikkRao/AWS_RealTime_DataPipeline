# AWS_RealTime_DataPipeline

Considerations:
-> AWS roles and configs set those up as in when you create services and give permisiions accordingly 
-> Used references:
    1) kafka-python module: https://kafka-python.readthedocs.io/en/master/index.html
    2) aws-glue-schema-registry: https://pypi.org/project/aws-glue-schema-registry/
-> Add layers to your lambda code for modules to work 

Project Overview:
I built a real-time data pipeline using AWS VPC, AWS Managed Streaming for Apache Kafka (MSK), AWS Lambda, Amazon ECS, and other AWS services to collect, process, and store live API data in an S3 bucket. The architecture ensures scalable, fault-tolerant data processing with schema validation through AWS Glue Schema Registry.

Let me walk you through the flow! ⬇️

🏗️ Architecture Setup (VPC & Networking):
-> Implemented my own VPC for advance security
-> 2 private subnets (AWS msk, lambda, ecs running in these) in different availability zones ap-south-1a, ap-south-1b
-> 2 public subnets each in one availability zone
-> 2 Route tables one for public subnets and another for private
-> internet gateway for 0.0.0.0/0 through igw
-> NAT gateway using one public subnet which is inside the public route table so that NAT can access internet (allocate elastic ip while creating)
-> Used this NAT gateway in private route table for internet access

This setup ensures that all services communicate securely within the VPC while accessing the internet only when necessary!

🔁 Data Flow:
1) Data source:
   -> Used Mastodon api which provides a live api endpoint for free this api will keep on sending new post data
   -> Check code for further understanding
2) API GATEWAY:
   -> The data generated by Mastodon is posted in the AWS API GATEWAY through post method
   -> created a REST Api through AWS GATEWAY and configured a post path
   -> changed the Integration request setting to post data to an SQS queue so that data can be polled from sqs
3) SQS:
   -> The data coming into Gateway is posted into SQS
   -> SQS is set as trigger for my Lambda Producer code
   -> Trigger is configured in such a way that either 50 messages will be batched or tigger will wait for 60sec and then invoke the lambda function
   -> Purpose for sqs is to reduce lambda invocations and bactching messages for cost cutting 
5) Lambda Producer:
   -> Has the code for producer logic of sending data recieved from SQS to the MSK cluster
   -> Have added layers for kafka-python and aws-glue-schema-registry modules
   -> Have used glue schema registry to serialize data into Avro format and compressed it to gzip for better output and cost cutting 
   -> logged information in cloudwatch
   -> configured necessary permission in execution role
6) MSK:
   -> Created a provisioned cluster
   -> set 2 brokers with minimum configuration to save cost each broker in different availabiltiy zone check the above vpc config details
   -> Can make a custom config or opt for serverless(handles everything for you) I chose the default settings
   -> Access permission I set to unauthorized and plain text(port 9092) for this project. Can use aceess control like IAM with TLS encryption will run on port 9094
   -> Broker endpoints generated are later used in producer and consumer code
7) Consumer code with ECS:
   -> Consumer code is a python script using kafka-python module
   -> Has my consumer logic with defining consumer group id and various other parameters
   -> derserialzes data with the schema id provided by producer
   -> This code is first build in docker then pushed as an Image in AWS ECR
   -> Using this image I have designed a Task defination taging this image and giving it appropriate role persmissions to work on
   -> Created an ECS cluster inside which spinning up a service running the task
   -> Very scalable since service can increase and decrease the number of tasks so depending upon number of partitions you can set number of tasks
   -> Data is consumed and stored in Kinesis Friehose
8) Kinesis Firehose:
   -> Main purpose is to reduce S3 api calls we dont want to just post each message individually to 3 as it is read so firehose batches it and sends them
   -> can connect with lambda if your data need transformations (skipped this in my project since my data is simple also can be cleaner or transformed later)
   -> can be configured such that the data being sent to s3 can use schema from glue and save files as parquet inbuilt feature of firehose, compression is also           possible instead eg: .json.gzip
   -> Used default batch settings, when met pushes data to designated S3 Location

This is the overall project and an overview of what i did and tried to implement. Got to learn a lot of things and hope this helps if someone trying to understand the project. many iterations and configurations can be changed according to needs and requirements i have also gone through it not applied some since it increases cost. 

Hope this helps.
    
   
